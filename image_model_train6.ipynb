{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1594393,
     "status": "ok",
     "timestamp": 1736074230702,
     "user": {
      "displayName": "Manik Chandra Biswas",
      "userId": "00149848136148089105"
     },
     "user_tz": -360
    },
    "id": "73DubWgI2sc8",
    "outputId": "b3163db5-0b21-4b48-b50a-e976b449a190"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50, InceptionV3, VGG16, EfficientNetB0, DenseNet121, MobileNet\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 1. Set Up Paths\n",
    "image_folder_path = '/content/drive/My Drive/instabot/ds/images'\n",
    "csv_path = '/content/drive/My Drive/instabot/ds/datasets.csv'\n",
    "model_save_path = '/content/drive/My Drive/models1/'\n",
    "import glob\n",
    "\n",
    "# Function to load all images and metadata\n",
    "def load_images_and_metadata(csv_path, image_folder_path):\n",
    "    data, labels, metadata = [], [], []\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Get all image files in the folder with specified extensions\n",
    "    all_images = glob.glob(os.path.join(image_folder_path, '*'))\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    all_images = [img for img in all_images if img.lower().endswith(valid_extensions)]\n",
    "\n",
    "    print(\"Found images:\")\n",
    "    for img in all_images:\n",
    "        print(img)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Match the image path using the `image` column\n",
    "        img_name = row['image']\n",
    "        img_path = os.path.join(image_folder_path, img_name)\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "        # Ensure the image exists in the folder\n",
    "        if img_path in all_images:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img_resized = cv2.resize(img, (128, 128))  # Resize to a consistent size\n",
    "                img_normalized = img_resized / 255.0  # Normalize pixel values\n",
    "                data.append(img_normalized)\n",
    "                labels.append(row['category'])\n",
    "                metadata.append({\n",
    "                    'objects': row['objects'],\n",
    "                    'distance': row['distance']\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Unable to read image - {img_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Image not found in folder - {img_name}\")\n",
    "\n",
    "    return np.array(data), np.array(labels), metadata\n",
    "# Load images and metadata\n",
    "\n",
    "X, y_labels, metadata = load_images_and_metadata(csv_path, image_folder_path)\n",
    "\n",
    "# Encode labels\n",
    "y_classes = sorted(set(y_labels))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(y_classes)}\n",
    "print(y_classes, label_to_idx)\n",
    "y = np.array([label_to_idx[label] for label in y_labels])\n",
    "y = to_categorical(y, num_classes=len(y_classes))\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TPU Strategy\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU\")\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU/GPU\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Pre-trained Models\n",
    "def create_pretrained_model(base_model, num_classes):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# List of Models\n",
    "model_classes = [\n",
    "    (EfficientNetB0, 'EfficientNetB0'),\n",
    "    (DenseNet121, 'DenseNet121'),\n",
    "    (MobileNet, 'MobileNet')\n",
    "]\n",
    "IMG_SIZE = 128\n",
    "# Train and Save Models\n",
    "for model_class, model_name in model_classes:\n",
    "    with strategy.scope():\n",
    "        print(f\"Preparing {model_name}...\")\n",
    "        base_model = model_class(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "        model = create_pretrained_model(base_model, len(y_classes))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=os.path.join(model_save_path, f\"{model_name}.keras\"),\n",
    "                                 save_best_only=True,\n",
    "                                 monitor='val_accuracy',\n",
    "                                 mode='max')\n",
    "\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "              epochs=100, batch_size=32, callbacks=[checkpoint])\n",
    "    print(f\"{model_name} training complete and saved to Google Drive.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Define Model Architectures\n",
    "def build_model_1():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(y_classes), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_model_2():\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(y_classes), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_model_3():\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(len(y_classes), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_model_4():\n",
    "    model = Sequential([\n",
    "        Conv2D(128, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.4),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.4),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(y_classes), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_model_5():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (5, 5), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.2),\n",
    "        Conv2D(64, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(len(y_classes), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def create_resnet(input_shape, num_classes):\n",
    "    base_model = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "def create_vgg16(input_shape, num_classes):\n",
    "    base_model = VGG16(weights=None, include_top=False, input_shape=input_shape)\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# 4. Train and Save Models\n",
    "models = [build_model_1, build_model_2, build_model_3, build_model_4, build_model_5]\n",
    "# model_names = [\"model_1.h5\", \"model_2.h5\", \"model_3.h5\", \"model_4.h5\", \"model_5.h5\"]\n",
    "model_names = [\"model_1.keras\", \"model_2.keras\", \"model_3.keras\", \"model_4.keras\", \"model_5.keras\"]\n",
    "for i, build_model in enumerate(models):\n",
    "    with strategy.scope():\n",
    "        model = build_model()\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=os.path.join(model_save_path, model_names[i]),\n",
    "                                 save_best_only=True,\n",
    "                                 monitor='val_accuracy',\n",
    "                                 mode='max')\n",
    "\n",
    "    print(f\"Training {model_names[i]}...\")\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "              epochs=100, batch_size=6, callbacks=[checkpoint])\n",
    "    print(f\"{model_names[i]} training complete and saved to Google Drive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9opgti-z6_Cu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWQfxgDOxKyL"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN9QysD+PvuHzak8/dRk5AN",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
